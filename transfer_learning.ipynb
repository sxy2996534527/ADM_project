{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sxy2996534527/ADM_project/blob/master/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jlPcSBcmGh71"
      },
      "source": [
        "This notebook has been prepared by Fabien Moutarde and Guillaume Devineau from MINES ParisTech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6w0YZN-XLwAy"
      },
      "source": [
        "NOTE: For the colab version of the notebook please make sure that you are using python3 and a **GPU accelerated backend** (select *GPU* as Hardware accelerator either in the *Edit > Notebook settings* or in the *Runtime > Change runtime type* menus for the english colab interface, or in the *Execution > Modifier le type d'ex√©cution* menus for the french colab interface)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YL9IosvfGh72"
      },
      "source": [
        "# Deep-Learning with Convolutional Neural Networks : Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E3rbRTynGh74"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SKGULbMBGh75"
      },
      "source": [
        "### 1.1 Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xk7AFDAWGh75"
      },
      "source": [
        "Convolutional Neural Networks (ConvNets) are powerful tools to create a mapping between an input domain, e.g. images, and an output domain, e.g. classes associated with the images.\n",
        "\n",
        "Creating effective neural networks for a given task requires design choices, which include the network topology, the optimization procedure and the regularization. This is time consuming and requires expert input. For this reason, people tend to use <i>(de facto)</i> standard neural networks already known to be effective for a given task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-yCJnTaJGh77"
      },
      "source": [
        "In this session, we will use a neural network, called **Inception v3** <code>[1]</code>, to classify images. The architecture of the Inception network is presented below.\n",
        "\n",
        "<img src=\"https://hackathonprojects.files.wordpress.com/2016/09/74911-image03.png\" alt=\"Inception\" width=\"95%\" />\n",
        "\n",
        "<code>[1] https://arxiv.org/pdf/1512.00567.pdf</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ul4BR3P9Gh78"
      },
      "source": [
        "### 1.2. Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7MI7y5o2Gh78"
      },
      "source": [
        "In order not to spend the whole practical session staring at the screen while the network is training, we will not train the network from scratch but we will rather take a different approach called **Transfer Learning**.\n",
        "\n",
        "When a model is trained on a given dataset for a given task, a knowledge useful for the task is accumulated. Transfer Learning is based on the hypothesis that this knowledge can -mostly- be reused for another similar dataset or task. In practice, this assumption is verified in a lot of cases.\n",
        "\n",
        "** Transfer learning is partcularly powerful when re-using a convNet pre-trained for image classification on the huge and general ImageNet dataset. This is because it has learnt and \"stored\" in its lower layers a very GENERAL-PURPOSE transformation of images into a hierarchy of features. This learnt transformation is general enough to be used as a foundation onto which one can stack new layers for EASILY LEARNING ANY FUNCTION OF IMAGES: categorization for totally different classes, regression of camera pose, etc... Moreover, because the re-used layers transform input images into a very \"pertinent\" feature space, transfer learning make it possible to learn WITH VERY FEW EXAMPLES PER CLASS (and rather few epochs of training), as illustrated on the plot below.**\n",
        "\n",
        "![](http://perso.mines-paristech.fr/fabien.moutarde/ES_LSML/TP_TransferLearning-ConvNet/transfer-learning_Caltech-veryFewExamplesPerClass.png) \n",
        "\n",
        "** The goal of this practical session is precisely to allow you to reproduce and check by yourself the above result.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSb-LuCWGh79"
      },
      "source": [
        "## 2. Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FxB7opRGGh7_"
      },
      "source": [
        "The notebook should work fine with both python 2 and python 3. If you have both, use python 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-TDjW91CGh8A",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "print('Your python version: {}'.format(sys.version_info.major))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E30yFp4VGh8E"
      },
      "source": [
        "### 2.1. (In case of emergency)\n",
        "\n",
        "The notebook should run smoothly, but in case of a missing module (<code>ImportError: No module named your_missing_module</code>), uncomment the cell below (comments start with a <code>#</code>) to install the python pip modules needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "czDst4v3Gh8E",
        "colab": {}
      },
      "source": [
        "#!{sys.executable} -m pip install -U keras --user\n",
        "#!{sys.executable} -m pip install -U np_utils --user\n",
        "!{sys.executable} -m pip install -U h5py --user  \n",
        "#!{sys.executable} -m pip install -U matplotlib --user\n",
        "#!{sys.executable} -m pip install -U seaborn --user\n",
        "#!{sys.executable} -m pip install -U scipy --user\n",
        "#!{sys.executable} -m pip install -U pygpu --user\n",
        "#!{sys.executable} -m pip install -U six --user\n",
        "#!{sys.executable} -m pip install -U tensorflow --user      # CPU-only version\n",
        "#!{sys.executable} -m pip install -U tensorflow-gpu --user  # GPU version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FzB9oxutGh8H"
      },
      "source": [
        "### 2.2. Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BDSboaFPGh8I"
      },
      "source": [
        "Let's import every module we'll use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZ9AKZ1uGh8J",
        "colab": {}
      },
      "source": [
        "USE_TENSORFLOW_AS_BACKEND = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-RBqVPyGh8L",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from scipy import misc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KfI8Oq5fGh8P",
        "colab": {}
      },
      "source": [
        "if USE_TENSORFLOW_AS_BACKEND:\n",
        "    os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "else:\n",
        "    os.environ['KERAS_BACKEND'] = 'theano'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wc3Sn4s4Gh8S",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VhsZ9gOhGh8W",
        "colab": {}
      },
      "source": [
        "if USE_TENSORFLOW_AS_BACKEND:\n",
        "    K.set_image_dim_ordering('tf')\n",
        "else:\n",
        "    K.set_image_dim_ordering('th')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u_EcI1YFGh8Y"
      },
      "source": [
        "Display the version of your modules (this notebook has successfully been tested with keras version 2.2.4 and tensorflow version 1.12.0. for instance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YQRMi_EFGh8Z",
        "colab": {}
      },
      "source": [
        "print('Your keras version: {}'.format(keras.__version__))\n",
        "if USE_TENSORFLOW_AS_BACKEND == True:\n",
        "    import tensorflow\n",
        "    print('Your tensorflow version: {}'.format(tensorflow.__version__))\n",
        "else:\n",
        "    import theano\n",
        "    print('Your theano version: {}'.format(theano.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8mJ1E80Gh8b"
      },
      "source": [
        "## 3. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZRJilbzIGh8c"
      },
      "source": [
        "We use a small image dataset called <code>CalTech 101 dataset</code> : it consists of pictures of objects belonging to 101 categories, with about 40 to 800 images per category; most categories have about 50 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ROoOZepHGh8d"
      },
      "source": [
        "### 3.1. Data download (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ypes8eqtGh8e"
      },
      "source": [
        "<b>Skip this subsection if your are working on a MINES ParisTech self-service machine</b>, as the dataset is already downloaded on MINES ParisTech self-service machines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sV_E6WvtGh8e"
      },
      "source": [
        "Otherwise, download the dataset on your own latop download this archive (*): http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz and extract it. For convenience, the dataset can also be  <a href=101_ObjectCategories.tar.gz> downloaded locally from here</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rT3NTPk4Gh8f",
        "colab": {}
      },
      "source": [
        "# --- only if you are on your own laptop ---\n",
        "\n",
        "# Uncomment to download the dataset\n",
        "# ...on Mines ParisTech computers:\n",
        "#!wget \"http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\" \n",
        "# ... or on google colab:\n",
        "![[ -f 101_ObjectCategories.tar.gz ]] || wget \"http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\"\n",
        "\n",
        "# Uncomment to extract the dataset\n",
        "# ...on Mines ParisTech computers:\n",
        "#!tar xzf 101_ObjectCategories.tar.gz\n",
        "# ... or on google colab:\n",
        "![[ -f 101_ObjectCategories ]] || tar xzf 101_ObjectCategories.tar.gz\n",
        "\n",
        "# Display files\n",
        "!ls 101_ObjectCategories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DBcEFdGyGh8i"
      },
      "source": [
        "Finally, <b>you should also change the value of</b> <code>dataset_path</code> in the <code>get_data()</code> function in code-cell below (section 3.2.) to include the correct path to the dataset you downloaded and extracted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0qgeQPP-Gh8j"
      },
      "source": [
        "### 3.2. Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "21ww3kuhGh8k"
      },
      "source": [
        "Quick functions to load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNVtwEhFGh8l",
        "colab": {}
      },
      "source": [
        "def load_batch_images(path, n_examples_train, n_examples_validation=2, max_imgs_by_class=30, nb_categories=4, resize_to_size=(224,224)):\n",
        "\n",
        "    images_train = []\n",
        "    labels_train = []\n",
        "    images_validation = []\n",
        "    labels_validation = []\n",
        "    \n",
        "    # Get categories\n",
        "    categories_list = os.listdir(path)\n",
        "    categories_list = [category for category in categories_list if category != '.DS_Store']\n",
        "    if isinstance(nb_categories, int) and len(categories_list) >= nb_categories:\n",
        "        categories_list = categories_list[0:nb_categories]\n",
        "\n",
        "    # For each category...\n",
        "    for idx, category in enumerate(categories_list):\n",
        "        # ... get images in category ...\n",
        "        images_list = os.listdir(os.path.join(path, category))        \n",
        "        if len(images_list) > max_imgs_by_class:\n",
        "            images_list = images_list[:max_imgs_by_class]\n",
        "        \n",
        "        indx = 0\n",
        "        for images in images_list[:n_examples_train + n_examples_validation]:                \n",
        "            # ... load them ...\n",
        "            img = load_img(os.path.join(path, category, images))  # as PIL images\n",
        "            img = img_to_array(img)                               # as numpy array\n",
        "            img = misc.imresize(img, resize_to_size)              # resize\n",
        "            # img = misc.imrotate(img, 180)\n",
        "            if indx < n_examples_train:\n",
        "                images_train.append(img)\n",
        "                labels_train.append(idx)\n",
        "            else:\n",
        "                images_validation.append(img)\n",
        "                labels_validation.append(idx)\n",
        "            indx += 1\n",
        "    \n",
        "    # Convert to one-hot\n",
        "    labels_train = np_utils.to_categorical(labels_train)\n",
        "    labels_validation = np_utils.to_categorical(labels_validation)\n",
        "\n",
        "    return images_train, labels_train, images_validation, labels_validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c4jmMoaCGh8n",
        "colab": {}
      },
      "source": [
        "def shuffle_dataset(a, b):\n",
        "    \"\"\"Shuffle the entire dataset\"\"\"\n",
        "    assert np.shape(a)[0] == np.shape(b)[0]\n",
        "    p = np.random.permutation(np.shape(a)[0])\n",
        "    return (a[p], b[p])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "37XgJOT8Gh8r",
        "colab": {}
      },
      "source": [
        "def get_data(samples_by_category=8, dataset_path=\"./101_ObjectCategories/\", nb_categories=4):\n",
        "    # NB: the dataset_path ABOVE SHOULD BE CORRECTED TO THE *ACTUAL PATH* WHERE YOU HAVE EXTRACTED 101_ObjectCategories\n",
        "    images_train, labels_train, images_validation, labels_validation = load_batch_images(path=dataset_path, n_examples_train=samples_by_category, n_examples_validation=20, max_imgs_by_class=30, nb_categories=nb_categories)\n",
        "\n",
        "    images_train = np.float64(images_train)\n",
        "    images_validation = np.float64(images_validation)\n",
        "\n",
        "    images_train = preprocess_input(images_train)  # substract the mean RGB channels of the imagenet dataset. This is because the Inception model we are using has been trained on a different dataset (imagenet dataset)\n",
        "    images_validation = preprocess_input(images_validation)\n",
        "\n",
        "    if not USE_TENSORFLOW_AS_BACKEND:\n",
        "        images_validation = images_validation.swapaxes(1, 3).swapaxes(2, 3)\n",
        "        images_train = images_train.swapaxes(1, 3).swapaxes(2, 3)\n",
        "\n",
        "    train = shuffle_dataset(np.asarray(images_train), labels_train)\n",
        "    validation = shuffle_dataset(np.asarray(images_validation), labels_validation)\n",
        "\n",
        "    images_train, labels_train = train\n",
        "    images_validation, labels_validation = validation\n",
        "\n",
        "    return images_train, labels_train, train, images_validation, labels_validation, validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiqJujj26ZWH",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. Sanity check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xMbr9J3JGh8v"
      },
      "source": [
        "Let's plot some images to check that the images are correctly loaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WbE786sVGh8v",
        "colab": {}
      },
      "source": [
        "def demo_plot():\n",
        "    demo_path = \"./101_ObjectCategories/\"\n",
        "    # NB: the dataset_path ABOVE SHOULD BE CORRECTED TO THE *ACTUAL PATH* WHERE YOU HAVE EXTRACTED 101_ObjectCategories\n",
        "    demo_data, _, _, _ = load_batch_images(path=demo_path, n_examples_train=8, n_examples_validation=20)\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(demo_data[0])\n",
        "    axarr[1,0].imshow(demo_data[1])\n",
        "    axarr[0,1].imshow(demo_data[27])\n",
        "    axarr[1,1].imshow(demo_data[31])\n",
        "    axarr[0,0].grid('off')\n",
        "    axarr[0,1].grid('off')\n",
        "    axarr[1,0].grid('off')\n",
        "    axarr[1,1].grid('off')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LXfKvo5sGh8y",
        "colab": {}
      },
      "source": [
        "demo_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-p6SpXeyGh80"
      },
      "source": [
        "Now, let's load the data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoGZ5hkt6ZWR",
        "colab_type": "text"
      },
      "source": [
        "### 3.4. Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DqIVoQaqGh81"
      },
      "source": [
        "To speed up the training, we will use only a few of the 101 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r30XTZULGh81",
        "colab": {}
      },
      "source": [
        "nb_categories = 4         # out of 101 classes\n",
        "samples_by_category = 10  # out of about 50 images for each category\n",
        "\n",
        "# Load the data!\n",
        "train_img, train_labels, train, val_img, val_labels, val = get_data(samples_by_category=samples_by_category, dataset_path='./101_ObjectCategories/', nb_categories=nb_categories)\n",
        "\n",
        "# Print the tensors shapes\n",
        "print('Shapes (train) :', train_img[0].shape, train_labels[0].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7kvEuOrXGh85"
      },
      "source": [
        "## 4. Transfer Learning\n",
        "\n",
        "### 4.1. Load a pre-trained model\n",
        "\n",
        "Create a neural network with the Inception architecture. Instead of learning the weights, we use existing weights obtained by (pre-)training the network on a dataset named Imagenet [2] for a classification task. Imagenet consists in 14,197,122 images associated with 1,000 classes, ranging from apples to lakes, radiators, or funambulists (http://image-net.org/explore?wnid=n00324978).\n",
        "\n",
        "We also tweak the network.\n",
        "\n",
        "Note the <code>keras</code> option <code>include_top=False</code>. On the one hand, one can view the first layers, which are close to the raw pixels input, as low-level signal processing units, since they mostly perform corner detection, shape detection or texture detection for instance. On the other hand, the last layers take already high-level representations of the input images and assign them to the desired classes.\n",
        "We keep the first layers and their weights since re-training them will take time but will likely provide very limited value. The high-levels we will later use are more specific to our use case.\n",
        "\n",
        "<code>[2] http://image-net.org</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cYf_MvHJGh85",
        "colab": {}
      },
      "source": [
        "# create the base pre-trained model\n",
        "# (the pretrained weights are stored in a file located at: https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5)\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kqMxjc3IGh88"
      },
      "source": [
        "Note the parameters count, and how many are learnable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LuujcPAoGh89",
        "colab": {}
      },
      "source": [
        "# print all the layers\n",
        "base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hroz3C6D6ZWa",
        "colab_type": "text"
      },
      "source": [
        "### 4.2. Tweak it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EH7rzExDGh8_"
      },
      "source": [
        "Adapt the network for our own task. Change the class number according to our classes count:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgEcA0HwGh9A",
        "colab": {}
      },
      "source": [
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "# and a logistic layer -- we have nb_categories classes\n",
        "predictions = Dense(nb_categories, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HVcWCKLBGh9D"
      },
      "source": [
        "\n",
        "** For now, let's not change (i.e. not train) the \"knowledge\" stored into weights during the pre-training on ImageNet. To this end, we \"freeze\" the weights of all pre-trained layers: only the final layers just added for adapting to our problem will be learnt:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VnOtnZtvGh9D",
        "colab": {}
      },
      "source": [
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# print adapted model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1d1pQ8zCGh9F"
      },
      "source": [
        "We now train the model.\n",
        "\n",
        "This means we minimize (using an <code>optimizer</code> algorithm) a function (the cross-entropy $H(p,q) = - \\sum_x p(x) \\log(q(x))$ where $p$ is the approximating distribution and $q$ is the true distribution).\n",
        "\n",
        "The minimization is performed iteratively, in several \"epochs\" (which include the forward pass, the loss function evaluation, the gradient backpropagation and the weights updates, using all examples of the training dataset).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYNP3N206ZWi",
        "colab_type": "text"
      },
      "source": [
        "### 4.3. Loss function & optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgkp2eA-Gh9G",
        "colab": {}
      },
      "source": [
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkNRqgST6ZWm",
        "colab_type": "text"
      },
      "source": [
        "### 4.4. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1sAy8uAGh9I",
        "colab": {}
      },
      "source": [
        "# train the model on the new data for a few epochs\n",
        "history = model.fit(train_img, train_labels, batch_size=32, validation_data=(val_img, val_labels), epochs=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_liVuzAhGh9K"
      },
      "source": [
        "Display the accuracy and loss on the validation/test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EYa2vc7yGh9L",
        "colab": {}
      },
      "source": [
        "print(model.metrics_names)\n",
        "print(model.evaluate(val_img, val_labels, verbose=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nOXH_mK6Gh9M",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss by epoch for %d samples per category' % samples_by_category)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy by epoch for %d samples per category' % samples_by_category)\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['train', 'test'], loc='right')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lM5-Ip7QGh9O"
      },
      "source": [
        "Note that the convNet was relatively quick to train by transfer learning on the new problem, despite its very large size. <br>\n",
        "**NB: if overfitting occurs (ie if validation accuracy stops increasing and even starts to diminish at some point during iterations), then add some regularization: dropout layer and/or a penalty term for high weights values in cost function.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RchMRL5XGh9P"
      },
      "source": [
        "## 5. What's left:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pSonALdfGh9P"
      },
      "source": [
        "**Plot the accuracy of the network after 12 epochs, depending on how many samples PER CLASS are used for the transfer learning (the goal is to check if you can get a result similar to that of the result plot showed at beginning of the notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-GaU7oEAGh9Q",
        "colab": {}
      },
      "source": [
        "accuracy = []\n",
        "numByCat = [1,2,3,4,5,7,9,11,13,15]\n",
        "numSamples = [4,8,12,16,20,28,36,44,52,60]\n",
        "for num in numByCat:\n",
        "  nb_categories = 4\n",
        "  samples_by_category = num \n",
        "\n",
        "  # Load the data!\n",
        "  train_img, train_labels, train, val_img, val_labels, val = get_data(samples_by_category=samples_by_category, dataset_path='./101_ObjectCategories/', nb_categories=nb_categories)\n",
        "  \n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  # let's add a fully-connected layer\n",
        "  x = Dense(32, activation='relu')(x)\n",
        "\n",
        "  # and a logistic layer -- we have nb_categories classes\n",
        "  predictions = Dense(nb_categories, activation='softmax')(x)\n",
        "  \n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "  \n",
        "  # first: train only the top layers (which were randomly initialized)\n",
        "  # i.e. freeze all convolutional InceptionV3 layers\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # compile the model (should be done *after* setting layers to non-trainable)\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "  \n",
        "  # data training\n",
        "  hist = model.fit(train_img, train_labels, batch_size=32, validation_data=(val_img, val_labels), epochs=12)\n",
        "  accuracy.append(hist.history['val_acc'][11])\n",
        "\n",
        "plt.plot(numSamples,accuracy)\n",
        "plt.plot(numSamples,accuracy,'bo')\n",
        "#plt.title('Model accuracy by epoch for %d samples per category' % samples_by_category)\n",
        "plt.ylabel('accuracy')\n",
        "#plt.legend(['train', 'test'], loc='right')\n",
        "plt.xlabel('number of examples')\n",
        "xlim(0,1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0w0Z4icSg7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "4e28de08-d7e4-4d2a-bda5-9ed1ff3b22b9"
      },
      "source": [
        "plt.plot(numSamples,accuracy)\n",
        "plt.plot(numSamples,accuracy,'bo')\n",
        "#plt.title('Model accuracy by epoch for %d samples per category' % samples_by_category)\n",
        "plt.ylabel('accuracy')\n",
        "#plt.legend(['train', 'test'], loc='right')\n",
        "plt.xlabel('number of examples')\n",
        "#plt.axis([0, 60, 0, 1])\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}